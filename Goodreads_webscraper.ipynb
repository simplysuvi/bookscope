{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce56215",
   "metadata": {},
   "source": [
    "# Web Scraping Goodreads: Exploring the World of Books\n",
    "\n",
    "Welcome to this web scraping project where extract data from [Goodreads](https://www.goodreads.com/). Welcome into the world of books, data, and insights. If you're a book lover like me, you're in for a treat! And if you're not, well, I believe this project might just inspire you to delve into the captivating realm of literature.\n",
    "\n",
    "In this project, we'll be harnessing the power of web scraping to extract a wealth of information from Goodreads, a treasure trove of book-related data. From book titles and authors to ratings, and more, Goodreads offers a vast reservoir of knowledge waiting to be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24181fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec56ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a user-agent header to identify your scraper\n",
    "user_agent = \"MyWebScraper/1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2301cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_titles = []\n",
    "authors = []\n",
    "avg_ratings = []\n",
    "ratings = []\n",
    "published_years = []\n",
    "editions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "710864c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on page 97: HTTPSConnectionPool(host='www.goodreads.com', port=443): Max retries exceeded with url: /search?page=97&q=fiction&search_type=books (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1480e60d0>: Failed to establish a new connection: [Errno 12] Cannot allocate memory'))\n",
      "Error on page 131: HTTPSConnectionPool(host='www.goodreads.com', port=443): Max retries exceeded with url: /search?page=131&q=fiction&search_type=books (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1488504d0>: Failed to establish a new connection: [Errno 12] Cannot allocate memory'))\n",
      "Error on page 135: HTTPSConnectionPool(host='www.goodreads.com', port=443): Max retries exceeded with url: /search?page=135&q=fiction&search_type=books (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x148a6fb50>: Failed to establish a new connection: [Errno 12] Cannot allocate memory'))\n",
      "Error on page 143: HTTPSConnectionPool(host='www.goodreads.com', port=443): Max retries exceeded with url: /search?page=143&q=fiction&search_type=books (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x148062910>: Failed to establish a new connection: [Errno 12] Cannot allocate memory'))\n",
      "Error on page 147: HTTPSConnectionPool(host='www.goodreads.com', port=443): Max retries exceeded with url: /search?page=147&q=fiction&search_type=books (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x147e7ca10>: Failed to establish a new connection: [Errno 12] Cannot allocate memory'))\n",
      "149/150 scraped.  Titles found: 1439\r"
     ]
    }
   ],
   "source": [
    "pages_to_scrape = 150\n",
    "\n",
    "# Specify the delay between requests in seconds (e.g., 2 seconds)\n",
    "request_delay = random.randint(2,6)\n",
    "\n",
    "# Loop through the pages to scrape\n",
    "for page in range(1,pages_to_scrape):\n",
    "    \n",
    "    # Construct the URL for the current page\n",
    "    url = \"https://www.goodreads.com/search?page=\" + str(page) + \"&q=fiction&search_type=books\"\n",
    "   \n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL with the user-agent header\n",
    "        headers = {\"User-Agent\": user_agent}\n",
    "        response = requests.get(url, headers=headers).text\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response, \"html.parser\")\n",
    "    \n",
    "        # Check for server errors or maintenance\n",
    "        if soup.title and \"service unavailable\" in soup.title.text.lower():\n",
    "            print(f\"Server error on page {page}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Select the table containing the list of books\n",
    "        table = soup.find_all(\"table\")[0]\n",
    "\n",
    "        # Loop through the rows of the table\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cells = row.find_all(\"td\")[1]\n",
    "\n",
    "            # Extract book title\n",
    "            title = cells.find(\"a\").find(\"span\").text\n",
    "            book_titles.append(title)\n",
    "\n",
    "            # Extract author's name\n",
    "            author = cells.find(\"a\", class_=\"authorName\").text\n",
    "            authors.append(author)\n",
    "            \n",
    "\n",
    "            #rating\n",
    "            all_ratings = cells.find_all('span', class_ = 'minirating')\n",
    "            all_ratings_text = all_ratings[0].text.strip()\n",
    "            pattern_2 = re.compile(r\"(\\d\\.?\\d*)\\savg\")\n",
    "            avg_ratings.append(pattern_2.search(all_ratings_text).group(1))\n",
    "\n",
    "            #n_ratings\n",
    "            pattern_4 = re.compile(r\"(?<=— )([\\d,]+)(?= ratings)\")\n",
    "            ratings_matches = pattern_4.search(all_ratings_text)\n",
    "            ratings.append(ratings_matches.group(1) if ratings_matches else 0)  \n",
    "\n",
    "\n",
    "#             # Extract average rating\n",
    "#             avg_rating = cells.find(\"span\", class_=\"greyText smallText uitext\").text.split()[0]\n",
    "#             avg_ratings.append(avg_rating)\n",
    "\n",
    "#             # Extract rating\n",
    "#             rating = cells.find(\"span\", class_=\"greyText smallText uitext\").text.split()[4]\n",
    "#             ratings.append(rating)\n",
    "\n",
    "            # Extract published year, handling cases where it may not be in the expected format\n",
    "            year_info = cells.find(\"span\", class_=\"greyText smallText uitext\").text.split()\n",
    "            year = None\n",
    "            for item in year_info:\n",
    "                if item.isdigit() and len(item) == 4:\n",
    "                    year = item\n",
    "                    break\n",
    "            if year:\n",
    "                published_years.append(year)\n",
    "            else:\n",
    "                published_years.append(0)  # Handle cases where year is not found\n",
    "\n",
    "            # Extract edition information\n",
    "            edition = cells.find(\"span\", class_=\"greyText smallText uitext\").text.split()[-2]\n",
    "            editions.append(edition)\n",
    "            print(f\"{page}/{pages_to_scrape} scraped.  Titles found: {len(book_titles)}\", end='\\r')\n",
    "\n",
    "        # Sleep to add a delay between requests\n",
    "        time.sleep(request_delay)\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle HTTP request errors (e.g., connection issues)\n",
    "        print(f\"Error on page {page}: {e}\")\n",
    "\n",
    "    except IndexError as e:\n",
    "        # Handle \"list index out of range\" error\n",
    "        print(f\"Index error on page {page}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle other unexpected errors\n",
    "        print(f\"Unexpected error on page {page}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c007fabe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# After scraping all pages, we create a DataFrame from the collected data\n",
    "data = {\n",
    "    \"Title\": book_titles,\n",
    "    \"Author\": authors,\n",
    "    \"Average Rating\": avg_ratings,\n",
    "    \"Rating\": ratings,\n",
    "    \"Year Published\": published_years,\n",
    "    \"Edition\": editions\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1af705b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Average Rating</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year Published</th>\n",
       "      <th>Edition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trigger Warning: Short Fictions and Disturbances</td>\n",
       "      <td>Neil Gaiman</td>\n",
       "      <td>3.92</td>\n",
       "      <td>61,194</td>\n",
       "      <td>2015</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Smoke and Mirrors: Short Fiction and Illusions</td>\n",
       "      <td>Neil Gaiman</td>\n",
       "      <td>4.02</td>\n",
       "      <td>71,896</td>\n",
       "      <td>1998</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fragile Things: Short Fictions and Wonders</td>\n",
       "      <td>Neil Gaiman</td>\n",
       "      <td>3.96</td>\n",
       "      <td>69,823</td>\n",
       "      <td>2006</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What She Left Behind: A Haunting and Heartbrea...</td>\n",
       "      <td>Ellen Marie Wiseman</td>\n",
       "      <td>3.99</td>\n",
       "      <td>57,408</td>\n",
       "      <td>2015</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Collected Fictions</td>\n",
       "      <td>Jorge Luis Borges</td>\n",
       "      <td>4.57</td>\n",
       "      <td>23,960</td>\n",
       "      <td>1998</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>Desire and Domestic Fiction: A Political Histo...</td>\n",
       "      <td>Nancy Armstrong</td>\n",
       "      <td>3.89</td>\n",
       "      <td>186</td>\n",
       "      <td>1987</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>Wandering Stars: An Anthology of Jewish Fantas...</td>\n",
       "      <td>Jack Dann</td>\n",
       "      <td>3.72</td>\n",
       "      <td>222</td>\n",
       "      <td>1974</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>The Book of Ramallah: A City in Short Fiction</td>\n",
       "      <td>Maya Abu al-Hayat</td>\n",
       "      <td>3.85</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>Sudden Fiction International: 60 Short-Short S...</td>\n",
       "      <td>Robert Shapard</td>\n",
       "      <td>3.75</td>\n",
       "      <td>348</td>\n",
       "      <td>1989</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>The Reincarnated Giant: An Anthology of Twenty...</td>\n",
       "      <td>Mingwei Song</td>\n",
       "      <td>3.69</td>\n",
       "      <td>58</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1439 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title               Author  \\\n",
       "0      Trigger Warning: Short Fictions and Disturbances          Neil Gaiman   \n",
       "1        Smoke and Mirrors: Short Fiction and Illusions          Neil Gaiman   \n",
       "2            Fragile Things: Short Fictions and Wonders          Neil Gaiman   \n",
       "3     What She Left Behind: A Haunting and Heartbrea...  Ellen Marie Wiseman   \n",
       "4                                    Collected Fictions    Jorge Luis Borges   \n",
       "...                                                 ...                  ...   \n",
       "1434  Desire and Domestic Fiction: A Political Histo...      Nancy Armstrong   \n",
       "1435  Wandering Stars: An Anthology of Jewish Fantas...            Jack Dann   \n",
       "1436      The Book of Ramallah: A City in Short Fiction    Maya Abu al-Hayat   \n",
       "1437  Sudden Fiction International: 60 Short-Short S...       Robert Shapard   \n",
       "1438  The Reincarnated Giant: An Anthology of Twenty...         Mingwei Song   \n",
       "\n",
       "     Average Rating  Rating Year Published Edition  \n",
       "0              3.92  61,194           2015      23  \n",
       "1              4.02  71,896           1998      99  \n",
       "2              3.96  69,823           2006      44  \n",
       "3              3.99  57,408           2015      54  \n",
       "4              4.57  23,960           1998      63  \n",
       "...             ...     ...            ...     ...  \n",
       "1434           3.89     186           1987      11  \n",
       "1435           3.72     222           1974      15  \n",
       "1436           3.85     100              0       2  \n",
       "1437           3.75     348           1989      11  \n",
       "1438           3.69      58           2017       1  \n",
       "\n",
       "[1439 rows x 6 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goodreads = pd.DataFrame(data)\n",
    "\n",
    "# Display the first five rows of the dataframe\n",
    "goodreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e11df4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "goodreads.to_csv(\"goodreads_books.csv\", index=False)\n",
    "# goodreads.to_csv(\"goodreads_books.csv\", mode='a', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
